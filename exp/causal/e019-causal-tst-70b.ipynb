{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzT66iezisGu"
   },
   "source": [
    "# 目的\n",
    "Llama3.1-70Bが動くかどうかのテスト\n",
    "\n",
    "ref: \n",
    "- https://github.com/unslothai/unsloth\n",
    "- https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing#scrollTo=6bZsfBuZDeCL\n",
    "- https://huggingface.co/blog/mlabonne/sft-llama3\n",
    "- https://huggingface.co/docs/trl/sft_trainer#accelerate-fine-tuning-2x-using-unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:11.641547Z",
     "iopub.status.busy": "2024-10-13T06:33:11.640832Z",
     "iopub.status.idle": "2024-10-13T06:33:11.657150Z",
     "shell.execute_reply": "2024-10-13T06:33:11.656283Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# -> だめ\n",
    "\n",
    "# pip uninstall torch\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# -> OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:11.661384Z",
     "iopub.status.busy": "2024-10-13T06:33:11.661043Z",
     "iopub.status.idle": "2024-10-13T06:33:11.670211Z",
     "shell.execute_reply": "2024-10-13T06:33:11.669375Z"
    },
    "id": "v7YkHHYsisGw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path setting\n",
    "EXP_NAME = \"e018-causal\"\n",
    "# MODEL_NAME = \"unsloth/gemma-2-9b-it-bnb-4bit\"\n",
    "# MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "# MODEL_NAME = \"nvidia/OpenMath2-Llama3.1-8B\"\n",
    "MODEL_NAME = \"unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit\"\n",
    "COMPETITION_NAME = \"eedi-mining-misconceptions-in-mathematics\"\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "DATASET_NAME = f\"{EXP_NAME}-{MODEL_NAME.split('/')[-1].replace('.', '-')}\"\n",
    "OUTPUT_PATH = f\"output/causal/{EXP_NAME}\"\n",
    "MODEL_OUTPUT_PATH = f\"{OUTPUT_PATH}/trained_model\"\n",
    "ENV_PATH = \"env_file\"\n",
    "\n",
    "# experiment parameter\n",
    "DEBUG = False\n",
    "TRAINING = True\n",
    "UPLOAD_DATA_TO_S3 = True\n",
    "UPLOAD_DATA_TO_KAGGLE = True\n",
    "REMOVE_LOCAL_FILE = False\n",
    "WANDB = True\n",
    "\n",
    "# USE_FOLD = 0\n",
    "# USE_DATA_RATE = 1.0\n",
    "# VALID_DATA_SIZE = 3000\n",
    "\n",
    "# model parameter\n",
    "TRAINING_MAX_LENGTH = 512\n",
    "SEED = 42\n",
    "EPOCH = 5\n",
    "LR = 2e-04\n",
    "TRAIN_BS = 1  # 2  # 4  # 16\n",
    "GRAD_ACC_STEP = 128 // TRAIN_BS  # 仮想的なバッチサイズはTRAIN_BS * GRAD_ACC_STEPとなる\n",
    "EVAL_BS = 1  # 2  # 4  # 16\n",
    "# NUM_LABELS = 3\n",
    "\n",
    "FREEZE_LAYERS = 24  # 16\n",
    "\n",
    "# rola parameter\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = LORA_R * 2\n",
    "LORA_BIAS = \"none\"\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = False  # 途中から再開する場合はTrueにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:11.674114Z",
     "iopub.status.busy": "2024-10-13T06:33:11.673741Z",
     "iopub.status.idle": "2024-10-13T06:33:12.097275Z",
     "shell.execute_reply": "2024-10-13T06:33:12.096140Z"
    },
    "id": "k-1rTdTmisGy",
    "outputId": "325e68ee-4d23-4a14-ff44-df1acc833ad3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 13 07:28:34 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-16GB           On  | 00000000:25:00.0 Off |                    0 |\n",
      "| N/A   23C    P0              25W / 250W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:12.101572Z",
     "iopub.status.busy": "2024-10-13T06:33:12.101212Z",
     "iopub.status.idle": "2024-10-13T06:33:12.388287Z",
     "shell.execute_reply": "2024-10-13T06:33:12.387154Z"
    },
    "id": "xdIcXQzqisGz",
    "outputId": "9de0ed12-bdfb-4fb2-c5b1-c6114afff122",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:12.392996Z",
     "iopub.status.busy": "2024-10-13T06:33:12.392489Z",
     "iopub.status.idle": "2024-10-13T06:33:12.404528Z",
     "shell.execute_reply": "2024-10-13T06:33:12.403715Z"
    },
    "id": "YeHMykyGisG0",
    "outputId": "7c447bf6-7758-40eb-aaee-4b35037d5567",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/eedi-mining-misconceptions-in-mathematics/exp/causal\n",
      "VastAi! Causal\n",
      "../../data\n",
      "/root/eedi-mining-misconceptions-in-mathematics/exp/causal\n",
      "VastAi! Causal\n",
      "../../output/causal/e018-causal\n",
      "/root/eedi-mining-misconceptions-in-mathematics/exp/causal\n",
      "VastAi! Causal\n",
      "../../output/causal/e018-causal/trained_model\n",
      "/root/eedi-mining-misconceptions-in-mathematics/exp/causal\n",
      "VastAi! Causal\n",
      "../../env_file\n"
     ]
    }
   ],
   "source": [
    "def resolve_path(base_path: str) -> str:\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    if cwd == f\"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode!\")\n",
    "        return f\"/notebooks/{COMPETITION_NAME}/{base_path}\"\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}\":\n",
    "        print(\"nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}/{COMPETITION_NAME}/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd == f\"/root/{COMPETITION_NAME}/exp/reranker\":\n",
    "        print(\"VastAi! Reranker\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd == f\"/root/{COMPETITION_NAME}/exp/retriever\":\n",
    "        print(\"VastAi! Retriever\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd == f\"/root/{COMPETITION_NAME}/exp/causal\":\n",
    "        print(\"VastAi! Causal\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd == f\"/root/{COMPETITION_NAME}\":\n",
    "        print(\"VastAi!\")\n",
    "        return base_path\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "DATA_PATH = resolve_path(DATA_PATH)\n",
    "print(DATA_PATH)\n",
    "OUTPUT_PATH = resolve_path(OUTPUT_PATH)\n",
    "print(OUTPUT_PATH)\n",
    "MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
    "print(MODEL_OUTPUT_PATH)\n",
    "ENV_PATH = resolve_path(ENV_PATH)\n",
    "print(ENV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:12.408548Z",
     "iopub.status.busy": "2024-10-13T06:33:12.408233Z",
     "iopub.status.idle": "2024-10-13T06:33:12.414403Z",
     "shell.execute_reply": "2024-10-13T06:33:12.413546Z"
    },
    "id": "E78wS3ypisG0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_dataset_name(dataset_name: str) -> None:\n",
    "    if len(dataset_name) < 6 or len(dataset_name) > 50:\n",
    "        raise Exception(\n",
    "            f\"データセットの文字列は6~50文字にしてください。現在{len(DATASET_NAME)}文字\"\n",
    "        )\n",
    "    if \"_\" in dataset_name or \".\" in dataset_name:\n",
    "        raise Exception(\"datasetの名称に'_'、'.'の使用は禁止です\")\n",
    "\n",
    "\n",
    "validate_dataset_name(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3uVqXs-isG1"
   },
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:12.418528Z",
     "iopub.status.busy": "2024-10-13T06:33:12.418208Z",
     "iopub.status.idle": "2024-10-13T06:33:18.320683Z",
     "shell.execute_reply": "2024-10-13T06:33:18.319485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:18.324837Z",
     "iopub.status.busy": "2024-10-13T06:33:18.324447Z",
     "iopub.status.idle": "2024-10-13T06:33:18.329106Z",
     "shell.execute_reply": "2024-10-13T06:33:18.328137Z"
    },
    "id": "fLkGFO2PisG1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "NUM_PROC = 16  # os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:18.332959Z",
     "iopub.status.busy": "2024-10-13T06:33:18.332777Z",
     "iopub.status.idle": "2024-10-13T06:33:18.337303Z",
     "shell.execute_reply": "2024-10-13T06:33:18.336358Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import trl\n",
    "\n",
    "assert torch.__version__ == \"2.4.0+cu121\"\n",
    "assert transformers.__version__ == \"4.44.2\"\n",
    "assert datasets.__version__ == \"3.0.0\"\n",
    "assert trl.__version__ == \"0.11.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:18.340945Z",
     "iopub.status.busy": "2024-10-13T06:33:18.340766Z",
     "iopub.status.idle": "2024-10-13T06:33:18.346749Z",
     "shell.execute_reply": "2024-10-13T06:33:18.345822Z"
    },
    "id": "o3ZVc72wisG2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seed the same seed to all\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:18.350205Z",
     "iopub.status.busy": "2024-10-13T06:33:18.349894Z",
     "iopub.status.idle": "2024-10-13T06:33:18.361650Z",
     "shell.execute_reply": "2024-10-13T06:33:18.360824Z"
    },
    "id": "8-7WkHGUisG2",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "e8711fa2-cd7f-4291-e280-0915341f5efc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(f\"{ENV_PATH}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohXfhbqXisG2"
   },
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:18.422329Z",
     "iopub.status.busy": "2024-10-13T06:33:18.421651Z",
     "iopub.status.idle": "2024-10-13T06:33:19.897795Z",
     "shell.execute_reply": "2024-10-13T06:33:19.896751Z"
    },
    "id": "KGR9aE2YisG2",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "95031754-538e-4af9-efc0-dec4d85afaf1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msinchir0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/eedi-mining-misconceptions-in-mathematics/exp/causal/wandb/run-20241013_072844-odvae1lp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics/runs/odvae1lp' target=\"_blank\">e018-causal</a></strong> to <a href='https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics' target=\"_blank\">https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics/runs/odvae1lp' target=\"_blank\">https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics/runs/odvae1lp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wandb'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if WANDB:\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=COMPETITION_NAME, name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\"\n",
    "\n",
    "REPORT_TO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tokenizer Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:19.902056Z",
     "iopub.status.busy": "2024-10-13T06:33:19.901704Z",
     "iopub.status.idle": "2024-10-13T06:33:48.562506Z",
     "shell.execute_reply": "2024-10-13T06:33:48.561442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.9.post3: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: Tesla V100-PCIE-16GB. Max memory: 15.773 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:1462: UserWarning: Current model requires 222299776.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING_MAX_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/loader.py:304\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py:1586\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;66;03m# Cannot be None, since HF now checks for the config\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bnb_config\n\u001b[0;32m-> 1586\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[1;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;66;03m# Return old flag\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_HUB_ENABLE_HF_TRANSFER\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m old_hf_transfer\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3909\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3906\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   3908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3909\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3912\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:86\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m         key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_not_convert\n\u001b[1;32m     84\u001b[0m     }\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=TRAINING_MAX_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:48.565740Z",
     "iopub.status.busy": "2024-10-13T06:33:48.565552Z",
     "iopub.status.idle": "2024-10-13T06:33:48.569521Z",
     "shell.execute_reply": "2024-10-13T06:33:48.568661Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train\n",
    "# tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxyhkSG4isG2"
   },
   "source": [
    "# Data Import & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:50.856424Z",
     "iopub.status.busy": "2024-10-13T06:33:50.855642Z",
     "iopub.status.idle": "2024-10-13T06:33:50.894362Z",
     "shell.execute_reply": "2024-10-13T06:33:50.893355Z"
    },
    "id": "JlTiKm6TisG2"
   },
   "outputs": [],
   "source": [
    "train_long = pl.read_csv(f\"{DATA_PATH}/train_long.csv\").filter(\n",
    "    (pl.col(\"CorrectAnswer\") != pl.col(\"AnswerAlphabet\"))\n",
    "    & (pl.col(\"MisconceptionId\").is_not_null())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:50.898516Z",
     "iopub.status.busy": "2024-10-13T06:33:50.898184Z",
     "iopub.status.idle": "2024-10-13T06:33:50.903048Z",
     "shell.execute_reply": "2024-10-13T06:33:50.902036Z"
    },
    "id": "6nkbuZVjisG3"
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train_long = train_long.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:50.906999Z",
     "iopub.status.busy": "2024-10-13T06:33:50.906660Z",
     "iopub.status.idle": "2024-10-13T06:33:50.917818Z",
     "shell.execute_reply": "2024-10-13T06:33:50.916842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4_370, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>QuestionId</th><th>ConstructName</th><th>SubjectName</th><th>QuestionText</th><th>CorrectAnswer</th><th>CorrectAnswerText</th><th>AnswerType</th><th>AnswerText</th><th>AllText</th><th>AnswerAlphabet</th><th>QuestionId_Answer</th><th>MisconceptionId</th><th>MisconceptionName</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;Use the order of operations to…</td><td>&quot;BIDMAS&quot;</td><td>&quot;\\[\n",
       "3 \\times 2+4-5\n",
       "\\]\n",
       "Where do …</td><td>&quot;A&quot;</td><td>&quot;\\( 3 \\times(2+4)-5 \\)&quot;</td><td>&quot;AnswerDText&quot;</td><td>&quot;Does not need brackets&quot;</td><td>&quot;Use the order of operations to…</td><td>&quot;D&quot;</td><td>&quot;0_D&quot;</td><td>1672</td><td>&quot;Confuses the order of operatio…</td></tr><tr><td>1000</td><td>&quot;Simplify an algebraic fraction…</td><td>&quot;Simplifying Algebraic Fraction…</td><td>&quot;Simplify the following, if pos…</td><td>&quot;B&quot;</td><td>&quot;\\( -1 \\)&quot;</td><td>&quot;AnswerAText&quot;</td><td>&quot;\\( t \\)&quot;</td><td>&quot;Simplify an algebraic fraction…</td><td>&quot;A&quot;</td><td>&quot;1000_A&quot;</td><td>891</td><td>&quot;Incorrectly cancels what they …</td></tr><tr><td>1000</td><td>&quot;Simplify an algebraic fraction…</td><td>&quot;Simplifying Algebraic Fraction…</td><td>&quot;Simplify the following, if pos…</td><td>&quot;B&quot;</td><td>&quot;\\( -1 \\)&quot;</td><td>&quot;AnswerCText&quot;</td><td>&quot;\\( 1 \\)&quot;</td><td>&quot;Simplify an algebraic fraction…</td><td>&quot;C&quot;</td><td>&quot;1000_C&quot;</td><td>891</td><td>&quot;Incorrectly cancels what they …</td></tr><tr><td>1000</td><td>&quot;Simplify an algebraic fraction…</td><td>&quot;Simplifying Algebraic Fraction…</td><td>&quot;Simplify the following, if pos…</td><td>&quot;B&quot;</td><td>&quot;\\( -1 \\)&quot;</td><td>&quot;AnswerDText&quot;</td><td>&quot;Does not simplify&quot;</td><td>&quot;Simplify an algebraic fraction…</td><td>&quot;D&quot;</td><td>&quot;1000_D&quot;</td><td>353</td><td>&quot;Does not recognise when one pa…</td></tr><tr><td>1001</td><td>&quot;Round numbers to two decimal p…</td><td>&quot;Rounding to Decimal Places&quot;</td><td>&quot;What is \\( \\mathbf{3 . 5 1 6 3…</td><td>&quot;B&quot;</td><td>&quot;\\( 3.52 \\)&quot;</td><td>&quot;AnswerAText&quot;</td><td>&quot;\\( 3.51 \\)&quot;</td><td>&quot;Round numbers to two decimal p…</td><td>&quot;A&quot;</td><td>&quot;1001_A&quot;</td><td>1379</td><td>&quot;Rounds down instead of up&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>99</td><td>&quot;Given the perimeter, work out …</td><td>&quot;Volume and Capacity Units&quot;</td><td>&quot;A regular pentagon has a total…</td><td>&quot;C&quot;</td><td>&quot;\\( 3.4 \\mathrm{~cm} \\)&quot;</td><td>&quot;AnswerBText&quot;</td><td>&quot;\\( 12 \\mathrm{~cm} \\)&quot;</td><td>&quot;Given the perimeter, work out …</td><td>&quot;B&quot;</td><td>&quot;99_B&quot;</td><td>1815</td><td>&quot;When given the perimeter of a …</td></tr><tr><td>99</td><td>&quot;Given the perimeter, work out …</td><td>&quot;Volume and Capacity Units&quot;</td><td>&quot;A regular pentagon has a total…</td><td>&quot;C&quot;</td><td>&quot;\\( 3.4 \\mathrm{~cm} \\)&quot;</td><td>&quot;AnswerDText&quot;</td><td>&quot;Not enough information&quot;</td><td>&quot;Given the perimeter, work out …</td><td>&quot;D&quot;</td><td>&quot;99_D&quot;</td><td>255</td><td>&quot;Does not know the meaning of t…</td></tr><tr><td>9</td><td>&quot;Identify horizontal translatio…</td><td>&quot;Transformations of functions i…</td><td>&quot;What transformation maps the g…</td><td>&quot;C&quot;</td><td>&quot;Translation by vector\n",
       "\\(\n",
       "\\left…</td><td>&quot;AnswerAText&quot;</td><td>&quot;Translation by vector\n",
       "\\(\n",
       "\\left…</td><td>&quot;Identify horizontal translatio…</td><td>&quot;A&quot;</td><td>&quot;9_A&quot;</td><td>1889</td><td>&quot;Believes f(x - a) translates t…</td></tr><tr><td>9</td><td>&quot;Identify horizontal translatio…</td><td>&quot;Transformations of functions i…</td><td>&quot;What transformation maps the g…</td><td>&quot;C&quot;</td><td>&quot;Translation by vector\n",
       "\\(\n",
       "\\left…</td><td>&quot;AnswerBText&quot;</td><td>&quot;Translation by vector\n",
       "\\(\n",
       "\\left…</td><td>&quot;Identify horizontal translatio…</td><td>&quot;B&quot;</td><td>&quot;9_B&quot;</td><td>1234</td><td>&quot;Believes f(x + a) translates t…</td></tr><tr><td>9</td><td>&quot;Identify horizontal translatio…</td><td>&quot;Transformations of functions i…</td><td>&quot;What transformation maps the g…</td><td>&quot;C&quot;</td><td>&quot;Translation by vector\n",
       "\\(\n",
       "\\left…</td><td>&quot;AnswerDText&quot;</td><td>&quot;Translation by vector\n",
       "\\(\n",
       "\\left…</td><td>&quot;Identify horizontal translatio…</td><td>&quot;D&quot;</td><td>&quot;9_D&quot;</td><td>1312</td><td>&quot;Believes f(x) + a translates t…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4_370, 13)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ QuestionI ┆ Construct ┆ SubjectNa ┆ QuestionT ┆ … ┆ AnswerAlp ┆ QuestionI ┆ Misconcep ┆ Misconce │\n",
       "│ d         ┆ Name      ┆ me        ┆ ext       ┆   ┆ habet     ┆ d_Answer  ┆ tionId    ┆ ptionNam │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ e        │\n",
       "│ i64       ┆ str       ┆ str       ┆ str       ┆   ┆ str       ┆ str       ┆ i64       ┆ ---      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 0         ┆ Use the   ┆ BIDMAS    ┆ \\[        ┆ … ┆ D         ┆ 0_D       ┆ 1672      ┆ Confuses │\n",
       "│           ┆ order of  ┆           ┆ 3 \\times  ┆   ┆           ┆           ┆           ┆ the      │\n",
       "│           ┆ operation ┆           ┆ 2+4-5     ┆   ┆           ┆           ┆           ┆ order of │\n",
       "│           ┆ s to…     ┆           ┆ \\]        ┆   ┆           ┆           ┆           ┆ operatio │\n",
       "│           ┆           ┆           ┆ Where do  ┆   ┆           ┆           ┆           ┆ …        │\n",
       "│           ┆           ┆           ┆ …         ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 1000      ┆ Simplify  ┆ Simplifyi ┆ Simplify  ┆ … ┆ A         ┆ 1000_A    ┆ 891       ┆ Incorrec │\n",
       "│           ┆ an        ┆ ng        ┆ the follo ┆   ┆           ┆           ┆           ┆ tly      │\n",
       "│           ┆ algebraic ┆ Algebraic ┆ wing, if  ┆   ┆           ┆           ┆           ┆ cancels  │\n",
       "│           ┆ fraction… ┆ Fraction… ┆ pos…      ┆   ┆           ┆           ┆           ┆ what     │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ they …   │\n",
       "│ 1000      ┆ Simplify  ┆ Simplifyi ┆ Simplify  ┆ … ┆ C         ┆ 1000_C    ┆ 891       ┆ Incorrec │\n",
       "│           ┆ an        ┆ ng        ┆ the follo ┆   ┆           ┆           ┆           ┆ tly      │\n",
       "│           ┆ algebraic ┆ Algebraic ┆ wing, if  ┆   ┆           ┆           ┆           ┆ cancels  │\n",
       "│           ┆ fraction… ┆ Fraction… ┆ pos…      ┆   ┆           ┆           ┆           ┆ what     │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ they …   │\n",
       "│ 1000      ┆ Simplify  ┆ Simplifyi ┆ Simplify  ┆ … ┆ D         ┆ 1000_D    ┆ 353       ┆ Does not │\n",
       "│           ┆ an        ┆ ng        ┆ the follo ┆   ┆           ┆           ┆           ┆ recognis │\n",
       "│           ┆ algebraic ┆ Algebraic ┆ wing, if  ┆   ┆           ┆           ┆           ┆ e when   │\n",
       "│           ┆ fraction… ┆ Fraction… ┆ pos…      ┆   ┆           ┆           ┆           ┆ one pa…  │\n",
       "│ 1001      ┆ Round     ┆ Rounding  ┆ What is   ┆ … ┆ A         ┆ 1001_A    ┆ 1379      ┆ Rounds   │\n",
       "│           ┆ numbers   ┆ to        ┆ \\(        ┆   ┆           ┆           ┆           ┆ down     │\n",
       "│           ┆ to two    ┆ Decimal   ┆ \\mathbf{3 ┆   ┆           ┆           ┆           ┆ instead  │\n",
       "│           ┆ decimal   ┆ Places    ┆ . 5 1 6   ┆   ┆           ┆           ┆           ┆ of up    │\n",
       "│           ┆ p…        ┆           ┆ 3…        ┆   ┆           ┆           ┆           ┆          │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 99        ┆ Given the ┆ Volume    ┆ A regular ┆ … ┆ B         ┆ 99_B      ┆ 1815      ┆ When     │\n",
       "│           ┆ perimeter ┆ and       ┆ pentagon  ┆   ┆           ┆           ┆           ┆ given    │\n",
       "│           ┆ , work    ┆ Capacity  ┆ has a     ┆   ┆           ┆           ┆           ┆ the peri │\n",
       "│           ┆ out …     ┆ Units     ┆ total…    ┆   ┆           ┆           ┆           ┆ meter of │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ a …      │\n",
       "│ 99        ┆ Given the ┆ Volume    ┆ A regular ┆ … ┆ D         ┆ 99_D      ┆ 255       ┆ Does not │\n",
       "│           ┆ perimeter ┆ and       ┆ pentagon  ┆   ┆           ┆           ┆           ┆ know the │\n",
       "│           ┆ , work    ┆ Capacity  ┆ has a     ┆   ┆           ┆           ┆           ┆ meaning  │\n",
       "│           ┆ out …     ┆ Units     ┆ total…    ┆   ┆           ┆           ┆           ┆ of t…    │\n",
       "│ 9         ┆ Identify  ┆ Transform ┆ What tran ┆ … ┆ A         ┆ 9_A       ┆ 1889      ┆ Believes │\n",
       "│           ┆ horizonta ┆ ations of ┆ sformatio ┆   ┆           ┆           ┆           ┆ f(x - a) │\n",
       "│           ┆ l transla ┆ functions ┆ n maps    ┆   ┆           ┆           ┆           ┆ translat │\n",
       "│           ┆ tio…      ┆ i…        ┆ the g…    ┆   ┆           ┆           ┆           ┆ es t…    │\n",
       "│ 9         ┆ Identify  ┆ Transform ┆ What tran ┆ … ┆ B         ┆ 9_B       ┆ 1234      ┆ Believes │\n",
       "│           ┆ horizonta ┆ ations of ┆ sformatio ┆   ┆           ┆           ┆           ┆ f(x + a) │\n",
       "│           ┆ l transla ┆ functions ┆ n maps    ┆   ┆           ┆           ┆           ┆ translat │\n",
       "│           ┆ tio…      ┆ i…        ┆ the g…    ┆   ┆           ┆           ┆           ┆ es t…    │\n",
       "│ 9         ┆ Identify  ┆ Transform ┆ What tran ┆ … ┆ D         ┆ 9_D       ┆ 1312      ┆ Believes │\n",
       "│           ┆ horizonta ┆ ations of ┆ sformatio ┆   ┆           ┆           ┆           ┆ f(x) + a │\n",
       "│           ┆ l transla ┆ functions ┆ n maps    ┆   ┆           ┆           ┆           ┆ translat │\n",
       "│           ┆ tio…      ┆ i…        ┆ the g…    ┆   ┆           ┆           ┆           ┆ es t…    │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:50.921982Z",
     "iopub.status.busy": "2024-10-13T06:33:50.921589Z",
     "iopub.status.idle": "2024-10-13T06:33:50.928589Z",
     "shell.execute_reply": "2024-10-13T06:33:50.927623Z"
    }
   },
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a mathematics expert. You are given a mathematics question, an incorrect answer, a correct answer, a construct name, and a subject name. Using these, identify what misconception exists in the incorrect answer.\",\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]\n",
    "\n",
    "# # TODO: CoT(think step by step)をどうFine-Tuningに反映させるべきかを調べる\n",
    "# alpaca_prompt = \"\"\"### Input:\n",
    "# Question: {question_text}\n",
    "# Incorrect Answer: {incorrect_answer_text}\n",
    "# Correct Answer: {correct_answer_text}\n",
    "# Construct Name: {construct_name}\n",
    "# Subject Name: {subject_name}\n",
    "\n",
    "# ### Response:\n",
    "# {misconception_name}\"\"\"\n",
    "\n",
    "# EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#     texts = []\n",
    "#     for (\n",
    "#         question_text,\n",
    "#         incorrect_answer_text,\n",
    "#         correct_answer_text,\n",
    "#         construct_name,\n",
    "#         subject_name,\n",
    "#         misconception_name,\n",
    "#     ) in zip(\n",
    "#         examples[\"QuestionText\"],\n",
    "#         examples[\"AnswerText\"],  # incorrect_answer_text\n",
    "#         examples[\"CorrectAnswerText\"],\n",
    "#         examples[\"ConstructName\"],\n",
    "#         examples[\"SubjectName\"],\n",
    "#         examples[\"MisconceptionName\"],\n",
    "#     ):\n",
    "#         # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "#         text = (\n",
    "#             alpaca_prompt.format(\n",
    "#                 question_text=question_text,\n",
    "#                 incorrect_answer_text=incorrect_answer_text,\n",
    "#                 correct_answer_text=correct_answer_text,\n",
    "#                 construct_name=construct_name,\n",
    "#                 subject_name=subject_name,\n",
    "#                 misconception_name=misconception_name,\n",
    "#             )\n",
    "#             + EOS_TOKEN\n",
    "#         )\n",
    "#         texts.append(text)\n",
    "#     return {\n",
    "#         \"text\": texts,\n",
    "#     }\n",
    "\n",
    "\n",
    "# train = Dataset.from_polars(train_long).map(\n",
    "#     formatting_prompts_func,\n",
    "#     batched=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:50.933201Z",
     "iopub.status.busy": "2024-10-13T06:33:50.932508Z",
     "iopub.status.idle": "2024-10-13T06:33:51.358663Z",
     "shell.execute_reply": "2024-10-13T06:33:51.357611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed0085ed5d54869af4927837eee6e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "QUESTION_TEMPLATE = \"\"\"Question: {question_text}\n",
    "Incorrect Answer: {incorrect_answer_text}\n",
    "Correct Answer: {correct_answer_text}\n",
    "Construct Name: {construct_name}\n",
    "Subject Name: {subject_name}\"\"\"\n",
    "\n",
    "\n",
    "class ApplyTemplateForTrain:\n",
    "    def make_masaages(self, question: str, answer: str) -> list[dict[str, str]]:\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a mathematics expert. You are given a mathematics question, an incorrect answer, a correct answer, a construct name, and a subject name. Using these, identify what misconception exists in the incorrect answer.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "\n",
    "    def apply_template(self, examples):\n",
    "        messages = []\n",
    "        for (\n",
    "            question_text,\n",
    "            incorrect_answer_text,\n",
    "            correct_answer_text,\n",
    "            construct_name,\n",
    "            subject_name,\n",
    "            misconception_name,\n",
    "        ) in zip(\n",
    "            examples[\"QuestionText\"],\n",
    "            examples[\"AnswerText\"],  # incorrect_answer_text\n",
    "            examples[\"CorrectAnswerText\"],\n",
    "            examples[\"ConstructName\"],\n",
    "            examples[\"SubjectName\"],\n",
    "            examples[\"MisconceptionName\"],\n",
    "        ):\n",
    "            message = self.make_masaages(\n",
    "                question=QUESTION_TEMPLATE.format(\n",
    "                    question_text=question_text,\n",
    "                    incorrect_answer_text=incorrect_answer_text,\n",
    "                    correct_answer_text=correct_answer_text,\n",
    "                    construct_name=construct_name,\n",
    "                    subject_name=subject_name,\n",
    "                ),\n",
    "                answer=misconception_name,\n",
    "            )\n",
    "            messages.append(message)\n",
    "\n",
    "        text = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                message, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            for message in messages\n",
    "        ]\n",
    "\n",
    "        return {\"text\": text}\n",
    "\n",
    "\n",
    "train = Dataset.from_polars(train_long).map(\n",
    "    ApplyTemplateForTrain().apply_template, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:51.362592Z",
     "iopub.status.busy": "2024-10-13T06:33:51.362405Z",
     "iopub.status.idle": "2024-10-13T06:33:51.367882Z",
     "shell.execute_reply": "2024-10-13T06:33:51.367087Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:51.372065Z",
     "iopub.status.busy": "2024-10-13T06:33:51.371391Z",
     "iopub.status.idle": "2024-10-13T06:33:51.377095Z",
     "shell.execute_reply": "2024-10-13T06:33:51.375849Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train[10][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:51.381217Z",
     "iopub.status.busy": "2024-10-13T06:33:51.381008Z",
     "iopub.status.idle": "2024-10-13T06:33:55.254477Z",
     "shell.execute_reply": "2024-10-13T06:33:55.253730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad8ba6ededf418e812396067b1cc9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=16):   0%|          | 0/4370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c939dddbbfac4649a6d11875e27ba8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=16):   0%|          | 0/4370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, valid = (\n",
    "    train.filter(lambda x: x[\"QuestionId\"] % 3 != 0, num_proc=NUM_PROC),\n",
    "    train.filter(lambda x: x[\"QuestionId\"] % 3 == 0, num_proc=NUM_PROC),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predict Check Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:55.258446Z",
     "iopub.status.busy": "2024-10-13T06:33:55.258266Z",
     "iopub.status.idle": "2024-10-13T06:33:55.262822Z",
     "shell.execute_reply": "2024-10-13T06:33:55.262297Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\" === Model Predict Check Before Training Start ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:55.266234Z",
     "iopub.status.busy": "2024-10-13T06:33:55.266068Z",
     "iopub.status.idle": "2024-10-13T06:33:55.269515Z",
     "shell.execute_reply": "2024-10-13T06:33:55.269016Z"
    }
   },
   "outputs": [],
   "source": [
    "# EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "# def formatting_prompts_func_predict(examples):\n",
    "#     texts = []\n",
    "#     for (\n",
    "#         question_text,\n",
    "#         incorrect_answer_text,\n",
    "#         correct_answer_text,\n",
    "#         construct_name,\n",
    "#         subject_name,\n",
    "#     ) in zip(\n",
    "#         examples[\"QuestionText\"],\n",
    "#         examples[\"AnswerText\"],  # incorrect_answer_text\n",
    "#         examples[\"CorrectAnswerText\"],\n",
    "#         examples[\"ConstructName\"],\n",
    "#         examples[\"SubjectName\"],\n",
    "#     ):\n",
    "#         # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "#         text = (\n",
    "#             alpaca_prompt.format(\n",
    "#                 question_text=question_text,\n",
    "#                 incorrect_answer_text=incorrect_answer_text,\n",
    "#                 correct_answer_text=correct_answer_text,\n",
    "#                 construct_name=construct_name,\n",
    "#                 subject_name=subject_name,\n",
    "#                 misconception_name=\"\",\n",
    "#             )\n",
    "#             + EOS_TOKEN\n",
    "#         )\n",
    "#         texts.append(text)\n",
    "#     return {\n",
    "#         \"text\": texts,\n",
    "#     }\n",
    "\n",
    "\n",
    "# valid_for_predict = valid.map(\n",
    "#     formatting_prompts_func_predict,\n",
    "#     batched=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:55.272860Z",
     "iopub.status.busy": "2024-10-13T06:33:55.272633Z",
     "iopub.status.idle": "2024-10-13T06:33:55.569372Z",
     "shell.execute_reply": "2024-10-13T06:33:55.568355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8007fe517d5a4a66a43f1da41faa8a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ApplyTemplateForPredict:\n",
    "    def make_masaages(self, question: str) -> list[dict[str, str]]:\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a mathematics expert. You are given a mathematics question, an incorrect answer, a correct answer, a construct name, and a subject name. Using these, identify what misconception exists in the incorrect answer.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "\n",
    "    def apply_template(self, examples):\n",
    "        messages = []\n",
    "        for (\n",
    "            question_text,\n",
    "            incorrect_answer_text,\n",
    "            correct_answer_text,\n",
    "            construct_name,\n",
    "            subject_name,\n",
    "        ) in zip(\n",
    "            examples[\"QuestionText\"],\n",
    "            examples[\"AnswerText\"],  # incorrect_answer_text\n",
    "            examples[\"CorrectAnswerText\"],\n",
    "            examples[\"ConstructName\"],\n",
    "            examples[\"SubjectName\"],\n",
    "        ):\n",
    "            message = self.make_masaages(\n",
    "                question=QUESTION_TEMPLATE.format(\n",
    "                    question_text=question_text,\n",
    "                    incorrect_answer_text=incorrect_answer_text,\n",
    "                    correct_answer_text=correct_answer_text,\n",
    "                    construct_name=construct_name,\n",
    "                    subject_name=subject_name,\n",
    "                )\n",
    "            )\n",
    "            messages.append(message)\n",
    "\n",
    "        text = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                message, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            for message in messages\n",
    "        ]\n",
    "\n",
    "        return {\"text\": text}\n",
    "\n",
    "\n",
    "valid_for_predict = valid.map(ApplyTemplateForPredict().apply_template, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:55.573152Z",
     "iopub.status.busy": "2024-10-13T06:33:55.572964Z",
     "iopub.status.idle": "2024-10-13T06:33:55.579404Z",
     "shell.execute_reply": "2024-10-13T06:33:55.578074Z"
    }
   },
   "outputs": [],
   "source": [
    "print(valid_for_predict[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:55.582797Z",
     "iopub.status.busy": "2024-10-13T06:33:55.582628Z",
     "iopub.status.idle": "2024-10-13T06:33:55.587738Z",
     "shell.execute_reply": "2024-10-13T06:33:55.586982Z"
    }
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "\n",
    "def predict(valid: Dataset, idx: int, model):\n",
    "    inputs = tokenizer(\n",
    "        valid[\"text\"][idx],\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, do_sample=True)\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "    print(\"\\n### Answer Misconception Name:\")\n",
    "    print(valid[\"MisconceptionName\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:55.590964Z",
     "iopub.status.busy": "2024-10-13T06:33:55.590799Z",
     "iopub.status.idle": "2024-10-13T06:33:55.595651Z",
     "shell.execute_reply": "2024-10-13T06:33:55.594895Z"
    }
   },
   "outputs": [],
   "source": [
    "print(valid_for_predict[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:55.599225Z",
     "iopub.status.busy": "2024-10-13T06:33:55.598716Z",
     "iopub.status.idle": "2024-10-13T06:33:59.102710Z",
     "shell.execute_reply": "2024-10-13T06:33:59.101120Z"
    }
   },
   "outputs": [],
   "source": [
    "predict(valid_for_predict, 0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:33:59.106733Z",
     "iopub.status.busy": "2024-10-13T06:33:59.106378Z",
     "iopub.status.idle": "2024-10-13T06:34:01.529990Z",
     "shell.execute_reply": "2024-10-13T06:34:01.528917Z"
    }
   },
   "outputs": [],
   "source": [
    "predict(valid_for_predict, 1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:34:01.533986Z",
     "iopub.status.busy": "2024-10-13T06:34:01.533799Z",
     "iopub.status.idle": "2024-10-13T06:34:03.939068Z",
     "shell.execute_reply": "2024-10-13T06:34:03.937955Z"
    }
   },
   "outputs": [],
   "source": [
    "predict(valid_for_predict, 2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:34:03.942858Z",
     "iopub.status.busy": "2024-10-13T06:34:03.942667Z",
     "iopub.status.idle": "2024-10-13T06:34:03.947822Z",
     "shell.execute_reply": "2024-10-13T06:34:03.946905Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\" === Model Predict Check Before Training End ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHoxXNPjisG3"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:34:03.951501Z",
     "iopub.status.busy": "2024-10-13T06:34:03.951329Z",
     "iopub.status.idle": "2024-10-13T06:34:11.622632Z",
     "shell.execute_reply": "2024-10-13T06:34:11.621562Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        # \"o_proj\",\n",
    "        # \"gate_proj\",\n",
    "        # \"up_proj\",\n",
    "        # \"down_proj\",\n",
    "        \"embed_tokens\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=SEED,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    "    layers_to_transform=[i for i in range(32) if i >= FREEZE_LAYERS],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:34:11.626961Z",
     "iopub.status.busy": "2024-10-13T06:34:11.626776Z",
     "iopub.status.idle": "2024-10-13T06:34:11.636635Z",
     "shell.execute_reply": "2024-10-13T06:34:11.635907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): ModulesToSaveWrapper(\n",
       "          (original_module): Embedding(128256, 4096)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Embedding(128256, 4096)\n",
       "          )\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "          (16-31): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T06:34:11.640859Z",
     "iopub.status.busy": "2024-10-13T06:34:11.640680Z",
     "iopub.status.idle": "2024-10-13T06:34:11.684348Z",
     "shell.execute_reply": "2024-10-13T06:34:11.683282Z"
    }
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_PATH,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEP,\n",
    "    eval_accumulation_steps=GRAD_ACC_STEP,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.1,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=2,\n",
    "    seed=SEED,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    report_to=REPORT_TO,\n",
    "    run_name=EXP_NAME,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:34:11.688107Z",
     "iopub.status.busy": "2024-10-13T06:34:11.687922Z",
     "iopub.status.idle": "2024-10-13T06:34:23.414014Z",
     "shell.execute_reply": "2024-10-13T06:34:23.413012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438a3338c1d24ac58343ca0565ab39ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/2910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05c0117e02b41efaab165df74453129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/1460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=valid,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=TRAINING_MAX_LENGTH,\n",
    "    dataset_num_proc=NUM_PROC,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:34:23.418706Z",
     "iopub.status.busy": "2024-10-13T06:34:23.418339Z",
     "iopub.status.idle": "2024-10-13T06:34:23.427870Z",
     "shell.execute_reply": "2024-10-13T06:34:23.426894Z"
    }
   },
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:34:23.432100Z",
     "iopub.status.busy": "2024-10-13T06:34:23.431764Z",
     "iopub.status.idle": "2024-10-13T06:34:23.438298Z",
     "shell.execute_reply": "2024-10-13T06:34:23.437352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/causal/e018-causal/trained_model'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T06:34:23.442368Z",
     "iopub.status.busy": "2024-10-13T06:34:23.442035Z",
     "iopub.status.idle": "2024-10-13T06:34:29.324851Z",
     "shell.execute_reply": "2024-10-13T06:34:29.323534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7999e2ed54024e2aa2cc9095aa82a8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting untrained tokens:   0%|          | 0/2910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 571.12 MiB is free. Process 3936233 has 15.21 GiB memory in use. Of the allocated memory 14.71 GiB is allocated by PyTorch, and 102.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAINING:\n\u001b[0;32m----> 2\u001b[0m     trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m FastLanguageModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mMODEL_OUTPUT_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/lora_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         max_seq_length\u001b[38;5;241m=\u001b[39mTRAINING_MAX_LENGTH,\n\u001b[1;32m      7\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# load_in_4bit=True,  # Use 4bit quantization to reduce memory usage. Can be False.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n",
      "File \u001b[0;32m<string>:145\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2192\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 571.12 MiB is free. Process 3936233 has 15.21 GiB memory in use. Of the allocated memory 14.71 GiB is allocated by PyTorch, and 102.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    trainer_stats = trainer.train()\n",
    "else:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_OUTPUT_PATH + \"/lora_model\",\n",
    "        max_seq_length=TRAINING_MAX_LENGTH,\n",
    "        dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "        # load_in_4bit=True,  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" === Model Predict Check After Training Start ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-12T10:53:25.313761Z",
     "iopub.status.busy": "2024-10-12T10:53:25.313573Z",
     "iopub.status.idle": "2024-10-12T10:53:26.069064Z",
     "shell.execute_reply": "2024-10-12T10:53:26.067998Z"
    }
   },
   "outputs": [],
   "source": [
    "predict(valid_for_predict, 0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T10:53:26.072961Z",
     "iopub.status.busy": "2024-10-12T10:53:26.072674Z",
     "iopub.status.idle": "2024-10-12T10:53:27.008441Z",
     "shell.execute_reply": "2024-10-12T10:53:27.007424Z"
    }
   },
   "outputs": [],
   "source": [
    "predict(valid_for_predict, 1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T10:53:27.012290Z",
     "iopub.status.busy": "2024-10-12T10:53:27.012112Z",
     "iopub.status.idle": "2024-10-12T10:53:27.715730Z",
     "shell.execute_reply": "2024-10-12T10:53:27.714699Z"
    }
   },
   "outputs": [],
   "source": [
    "predict(valid_for_predict, 2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" === Model Predict Check After Training End ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-12T10:53:27.718430Z",
     "iopub.status.busy": "2024-10-12T10:53:27.718249Z",
     "iopub.status.idle": "2024-10-12T10:53:28.222818Z",
     "shell.execute_reply": "2024-10-12T10:53:28.221831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output/causal/e014-causal/trained_model/lora_model/tokenizer_config.json',\n",
       " 'output/causal/e014-causal/trained_model/lora_model/special_tokens_map.json',\n",
       " 'output/causal/e014-causal/trained_model/lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(f\"{MODEL_OUTPUT_PATH}/lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(f\"{MODEL_OUTPUT_PATH}/lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt4jBGyxisG7"
   },
   "source": [
    "# AWSへのアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-12T10:53:28.226695Z",
     "iopub.status.busy": "2024-10-12T10:53:28.226514Z",
     "iopub.status.idle": "2024-10-12T10:53:57.583798Z",
     "shell.execute_reply": "2024-10-12T10:53:57.582531Z"
    },
    "id": "lupk0lEqisG7",
    "outputId": "c9a3fd96-b5c2-4c0c-8fe6-974459216930",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# S3へのアップロード\n",
    "if not DEBUG and UPLOAD_DATA_TO_S3:\n",
    "    # uninstall\n",
    "    !sudo rm /usr/bin/aws\n",
    "    !sudo rm /usr/bin/aws_completer\n",
    "    !sudo rm -rf /usr/local/aws-cli\n",
    "\n",
    "    # install\n",
    "    !curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "    !unzip -o -qq awscliv2.zip\n",
    "    !sudo ./aws/install --update\n",
    "\n",
    "    # upload\n",
    "    output_name = MODEL_OUTPUT_PATH.split(\"/\")[-1]\n",
    "    os.system(\n",
    "        f\"aws s3 cp --recursive {MODEL_OUTPUT_PATH} s3://{COMPETITION_NAME}/trained_model/{output_name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T10:53:57.588460Z",
     "iopub.status.busy": "2024-10-12T10:53:57.588072Z",
     "iopub.status.idle": "2024-10-12T10:53:57.593681Z",
     "shell.execute_reply": "2024-10-12T10:53:57.592695Z"
    },
    "id": "oA3GNoNAisG7"
   },
   "outputs": [],
   "source": [
    "# ダウンロード（参考）\n",
    "# !sudo rm /usr/bin/aws\n",
    "# !sudo rm /usr/bin/aws_completer\n",
    "# !sudo rm -rf /usr/local/aws-cli\n",
    "\n",
    "# !curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "# !unzip -o -qq awscliv2.zip\n",
    "# !sudo ./aws/install --update\n",
    "\n",
    "# !aws s3 cp --recursive s3://automated-essay-scoring/trained_model/e005-regression /notebooks/automated_essay_scoring/trained_models/e005-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XyyX1JqisG7"
   },
   "source": [
    "# Kaggle Datasetへのupload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-12T10:53:57.597737Z",
     "iopub.status.busy": "2024-10-12T10:53:57.597397Z",
     "iopub.status.idle": "2024-10-12T10:53:57.612438Z",
     "shell.execute_reply": "2024-10-12T10:53:57.611572Z"
    },
    "id": "2yljmioJisG7",
    "outputId": "5eb1319b-0b11-4317-f01b-cf227b3232c2",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"mkdir -p ~/.kaggle/\")\n",
    "os.system(f\"cp {ENV_PATH}/kaggle.json ~/.kaggle/\")\n",
    "os.system(\"chmod 600 ~/.kaggle/kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-12T10:53:57.616575Z",
     "iopub.status.busy": "2024-10-12T10:53:57.616234Z",
     "iopub.status.idle": "2024-10-12T10:54:04.111665Z",
     "shell.execute_reply": "2024-10-12T10:54:04.110141Z"
    },
    "id": "EBXdM74XisG7",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "290aef28-6839-4562-bca9-53f774396d12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Dataset name:e017-causal-Meta-Llama-3-1-8B-Instruct-bnb-4bit, output_dir:../../output/causal/e017-causal/trained_model\n",
      "Starting upload for file checkpoint-110.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62.9M/62.9M [00:03<00:00, 20.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: checkpoint-110.tar (63MB)\n",
      "Starting upload for file lora_model.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.8M/26.8M [00:02<00:00, 13.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: lora_model.tar (27MB)\n"
     ]
    }
   ],
   "source": [
    "if not DEBUG and UPLOAD_DATA_TO_KAGGLE:\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "        dataset_metadata = {}\n",
    "        dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "        dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "        dataset_metadata[\"title\"] = dataset_name\n",
    "        with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "            json.dump(dataset_metadata, f, indent=4)\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "    print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{MODEL_OUTPUT_PATH}\")\n",
    "    dataset_create_new(dataset_name=DATASET_NAME, upload_dir=MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6KFg2LNisG8"
   },
   "source": [
    "# ローカルからのデータの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T10:54:04.115510Z",
     "iopub.status.busy": "2024-10-12T10:54:04.114947Z",
     "iopub.status.idle": "2024-10-12T10:54:04.120592Z",
     "shell.execute_reply": "2024-10-12T10:54:04.119306Z"
    },
    "id": "tFhHkzi3isG8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if not DEBUG and REMOVE_LOCAL_FILE:\n",
    "#     # ローカルからは削除\n",
    "#     os.system(f\"rm -rf {MODEL_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ae1f8a8d15114a55a81d98bbc4563ffa",
      "24846464fd9949488c69b7c8262b8176",
      "a6025d29e90449f49116bb74afc3b591",
      "fc246d5347ee4fc782a9f42e1fb38ffe",
      "e0a0206188b34cb0bbbb62921d4edfb4",
      "b3b2143d95a04a8ab99d72d3ede6012a",
      "13c10dcfeeef4c1a882f260bf1628be2",
      "83014dfe9f554182ab1933951c39f693"
     ]
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-12T10:54:04.124384Z",
     "iopub.status.busy": "2024-10-12T10:54:04.123544Z",
     "iopub.status.idle": "2024-10-12T10:54:07.384661Z",
     "shell.execute_reply": "2024-10-12T10:54:07.383472Z"
    },
    "id": "bL2CHIBOisG8",
    "outputId": "5b5cbe19-9070-4319-8867-637c31771b00",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "autoscroll": "auto",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-12T10:54:07.387892Z",
     "iopub.status.busy": "2024-10-12T10:54:07.387530Z",
     "iopub.status.idle": "2024-10-12T10:54:07.393539Z",
     "shell.execute_reply": "2024-10-12T10:54:07.392388Z"
    },
    "id": "ohN9BBugisG8",
    "outputId": "28bd693d-d1fe-4c50-d81b-3412267d48a4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"finish Notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mH-uPM5EuoqS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06ca518a3b0f496a93027fbdd9d81ca2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0eae588571ba4ea7abe0973ce2bd21e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1074d240df184bac93aed7e75b4b507f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11158920668648e994d2ca1e531b8ba7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "117c238656a648d7b29da8296ee43474": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_06ca518a3b0f496a93027fbdd9d81ca2",
       "placeholder": "​",
       "style": "IPY_MODEL_e9de009225594d97af0ab90d9e2d7064",
       "tabbable": null,
       "tooltip": null,
       "value": " 4370/4370 [00:00&lt;00:00, 18846.74 examples/s]"
      }
     },
     "1728dfce451d467e9933f1788ac53852": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "18403d62a8404378ba180feb18005e4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1a0a16968e254bc79d55f43e5b80e6fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3398dbe96cc546489e7e6645240dacf6",
       "max": 1460,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5e7ef871fb3044689ec8f45f880e4eb8",
       "tabbable": null,
       "tooltip": null,
       "value": 1460
      }
     },
     "1c06f5de96e347fb8f9856c5c2ab3544": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "26b1d92da01b4c10938da135794b0c5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_be0d7f476de745ee907f3a80d9f0c458",
       "placeholder": "​",
       "style": "IPY_MODEL_c1f48b73c26148e29ac0aef46072fa21",
       "tabbable": null,
       "tooltip": null,
       "value": "Map (num_proc=16): 100%"
      }
     },
     "29807d324f944892ac87c6a5eaa06192": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2afcfbd669cb497a819450ec23587552": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ddccc1a2b62f4b9d9d1a670228b982f6",
       "placeholder": "​",
       "style": "IPY_MODEL_f0c18510d76440ca895fe40277de02b4",
       "tabbable": null,
       "tooltip": null,
       "value": "Filter (num_proc=16): 100%"
      }
     },
     "2cf645ee721c4ea894562e41c75ea815": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b4a627a492904ecca863deb88a3e6702",
       "max": 4370,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_efa3f1b88d8d45da934229842a554550",
       "tabbable": null,
       "tooltip": null,
       "value": 4370
      }
     },
     "2e7275938c7f49b28fe54c0b028c2565": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f07b640e1c4544a38cb22b30caf0664e",
       "max": 2910,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7f3f528136974da4afdb85dcf06e3c99",
       "tabbable": null,
       "tooltip": null,
       "value": 2910
      }
     },
     "2f3f0091278e451fa962e89729bc970d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "319e8f31f007433189581c12e29b3a75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "31f7c6dfdaa6475294d8bca905eadbda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3398dbe96cc546489e7e6645240dacf6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "37f965c0d1594a0a9e6e46c4c74cb59c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ca95c0830244e7f927cfaff94e6d360": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_29807d324f944892ac87c6a5eaa06192",
       "placeholder": "​",
       "style": "IPY_MODEL_18403d62a8404378ba180feb18005e4a",
       "tabbable": null,
       "tooltip": null,
       "value": " 4370/4370 [00:00&lt;00:00, 6562.63 examples/s]"
      }
     },
     "465890413dc4484bb339e45ea5eae3c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "489457ccdfee437590550fce8cc22a65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a5d6f0c3df4b41239912f69b7f76a05d",
       "placeholder": "​",
       "style": "IPY_MODEL_0eae588571ba4ea7abe0973ce2bd21e5",
       "tabbable": null,
       "tooltip": null,
       "value": "Map: 100%"
      }
     },
     "5e7ef871fb3044689ec8f45f880e4eb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "60d93dff6caa4bd88ec7540450873316": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_edccaa95984045af9e80c54cef838c92",
       "placeholder": "​",
       "style": "IPY_MODEL_2f3f0091278e451fa962e89729bc970d",
       "tabbable": null,
       "tooltip": null,
       "value": "Map: 100%"
      }
     },
     "62aa9c11c87e4b65a9b08d7339a2c65b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "679cfc6de2384219a063446435fb9a11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_489457ccdfee437590550fce8cc22a65",
        "IPY_MODEL_98d53ce266f545699dc4c8b0f2c977ec",
        "IPY_MODEL_731941cdc63341259729b6923e9fc5e9"
       ],
       "layout": "IPY_MODEL_11158920668648e994d2ca1e531b8ba7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6c1d39236fd448208a2a67aadc4c2c92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6e3883feeedb4ecf8592388dfc6d465e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "731941cdc63341259729b6923e9fc5e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eeab1a8350974ba9a38ea88a7717d08e",
       "placeholder": "​",
       "style": "IPY_MODEL_b3b270d954a74cc5b751c62bfc390a38",
       "tabbable": null,
       "tooltip": null,
       "value": " 1460/1460 [00:00&lt;00:00, 11881.77 examples/s]"
      }
     },
     "7664b1288ddc4a12b264b692fa4cd44e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7cd5255247134826b89c98ea81d8caa9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7ecd8cd301f541a49bc3a36117eb0f4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f99516e2441c4c649e3b1dd36f046395",
        "IPY_MODEL_1a0a16968e254bc79d55f43e5b80e6fd",
        "IPY_MODEL_9f6a75b5f9a44f70b5a641863c48c469"
       ],
       "layout": "IPY_MODEL_465890413dc4484bb339e45ea5eae3c3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7f3f528136974da4afdb85dcf06e3c99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8532dcee5ba44d71b5db6a5a280f8bc9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_26b1d92da01b4c10938da135794b0c5b",
        "IPY_MODEL_2e7275938c7f49b28fe54c0b028c2565",
        "IPY_MODEL_97c91f13efff4a88a12e8ca1eb534f56"
       ],
       "layout": "IPY_MODEL_e4abf9c42308457a932c045cfae9a4f0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "860a6cda02d94be38a53aed85a0d6828": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_60d93dff6caa4bd88ec7540450873316",
        "IPY_MODEL_fba5c97dd2da4610a657cdd1eb1d0dec",
        "IPY_MODEL_117c238656a648d7b29da8296ee43474"
       ],
       "layout": "IPY_MODEL_de0d578526e44a08918da7e7ddfc2e2a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "877de7b6d2da4ac89bfed2e038d67dc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "978cef35ebe24c839406f12bf0512388": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6c1d39236fd448208a2a67aadc4c2c92",
       "max": 4370,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b1ad66c57fe443f9a3bb9fddce60ebbc",
       "tabbable": null,
       "tooltip": null,
       "value": 4370
      }
     },
     "97c91f13efff4a88a12e8ca1eb534f56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_62aa9c11c87e4b65a9b08d7339a2c65b",
       "placeholder": "​",
       "style": "IPY_MODEL_1728dfce451d467e9933f1788ac53852",
       "tabbable": null,
       "tooltip": null,
       "value": " 2910/2910 [00:04&lt;00:00, 777.83 examples/s]"
      }
     },
     "98d53ce266f545699dc4c8b0f2c977ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6e3883feeedb4ecf8592388dfc6d465e",
       "max": 1460,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1c06f5de96e347fb8f9856c5c2ab3544",
       "tabbable": null,
       "tooltip": null,
       "value": 1460
      }
     },
     "9eb8d636574f446793ddce31a2e4efee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_319e8f31f007433189581c12e29b3a75",
       "placeholder": "​",
       "style": "IPY_MODEL_cc9329bb95de4bff9d5f86d341b004cf",
       "tabbable": null,
       "tooltip": null,
       "value": " 4370/4370 [00:00&lt;00:00, 6171.42 examples/s]"
      }
     },
     "9f6a75b5f9a44f70b5a641863c48c469": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7664b1288ddc4a12b264b692fa4cd44e",
       "placeholder": "​",
       "style": "IPY_MODEL_7cd5255247134826b89c98ea81d8caa9",
       "tabbable": null,
       "tooltip": null,
       "value": " 1460/1460 [00:04&lt;00:00, 403.79 examples/s]"
      }
     },
     "a5d6f0c3df4b41239912f69b7f76a05d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1ad66c57fe443f9a3bb9fddce60ebbc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b302589e33d440a4bca18f92fdccc516": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2afcfbd669cb497a819450ec23587552",
        "IPY_MODEL_2cf645ee721c4ea894562e41c75ea815",
        "IPY_MODEL_3ca95c0830244e7f927cfaff94e6d360"
       ],
       "layout": "IPY_MODEL_bd3f9eda00094504915f74f184428a58",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b3b270d954a74cc5b751c62bfc390a38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b4a627a492904ecca863deb88a3e6702": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b908a5234f6548d18fbb7a8a3d403e1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bd3f9eda00094504915f74f184428a58": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be0d7f476de745ee907f3a80d9f0c458": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c14c4059133d42d7af2eef835920b2c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c1f48b73c26148e29ac0aef46072fa21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c9bc9fa0e6264b9db5b33e97ae307b21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c14c4059133d42d7af2eef835920b2c3",
       "placeholder": "​",
       "style": "IPY_MODEL_d4ad80f110154e30a38a62da22e9488c",
       "tabbable": null,
       "tooltip": null,
       "value": "Filter (num_proc=16): 100%"
      }
     },
     "cc9329bb95de4bff9d5f86d341b004cf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cd1a8c544c2349eb9b175b98489ad798": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c9bc9fa0e6264b9db5b33e97ae307b21",
        "IPY_MODEL_978cef35ebe24c839406f12bf0512388",
        "IPY_MODEL_9eb8d636574f446793ddce31a2e4efee"
       ],
       "layout": "IPY_MODEL_37f965c0d1594a0a9e6e46c4c74cb59c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d4ad80f110154e30a38a62da22e9488c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ddccc1a2b62f4b9d9d1a670228b982f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de0d578526e44a08918da7e7ddfc2e2a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4abf9c42308457a932c045cfae9a4f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9de009225594d97af0ab90d9e2d7064": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "edccaa95984045af9e80c54cef838c92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eeab1a8350974ba9a38ea88a7717d08e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "efa3f1b88d8d45da934229842a554550": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f07b640e1c4544a38cb22b30caf0664e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0c18510d76440ca895fe40277de02b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f99516e2441c4c649e3b1dd36f046395": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_31f7c6dfdaa6475294d8bca905eadbda",
       "placeholder": "​",
       "style": "IPY_MODEL_b908a5234f6548d18fbb7a8a3d403e1e",
       "tabbable": null,
       "tooltip": null,
       "value": "Map (num_proc=16): 100%"
      }
     },
     "fba5c97dd2da4610a657cdd1eb1d0dec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1074d240df184bac93aed7e75b4b507f",
       "max": 4370,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_877de7b6d2da4ac89bfed2e038d67dc8",
       "tabbable": null,
       "tooltip": null,
       "value": 4370
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
