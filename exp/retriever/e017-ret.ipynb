{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71acc131",
   "metadata": {
    "papermill": {
     "duration": 0.011532,
     "end_time": "2024-09-14T01:45:57.074016",
     "exception": false,
     "start_time": "2024-09-14T01:45:57.062484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 目的\n",
    "stella_en_1.5B_v5をfine-tuningする\n",
    "\n",
    "ref: https://sbert.net/docs/sentence_transformer/training_overview.html#trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c5e900",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:22.755166Z",
     "iopub.status.busy": "2024-11-04T08:57:22.754707Z",
     "iopub.status.idle": "2024-11-04T08:57:23.100071Z",
     "shell.execute_reply": "2024-11-04T08:57:23.098068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  9 08:07:58 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090 Ti     On  |   00000000:05:00.0 Off |                  Off |\n",
      "|  0%   59C    P8             26W /  300W |    9893MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc8a91",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c063a5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:23.105430Z",
     "iopub.status.busy": "2024-11-04T08:57:23.104944Z",
     "iopub.status.idle": "2024-11-04T08:57:23.114942Z",
     "shell.execute_reply": "2024-11-04T08:57:23.113823Z"
    }
   },
   "outputs": [],
   "source": [
    "EXP_NAME = \"e016-ret-stella\"\n",
    "DATA_PATH = \"data\"\n",
    "MODEL_NAME = \"dunzhang/stella_en_1.5B_v5\"\n",
    "ENV_PATH = \"env_file\"\n",
    "COMPETITION_NAME = \"eedi-mining-misconceptions-in-mathematics\"\n",
    "RETRIVED_FILE_PATH = (\n",
    "    \"output/retriever/e003-ret-bge/e003-ret-bge-ret25-map0.1841-recall0.5506.csv\"\n",
    ")\n",
    "\n",
    "# RETRIVED_EX_FILE_PATH = \"output/retriever/e013-ret-bge-prepare/e013-ret-bge-prepare-ret25-map0.4374-recall0.8410.csv\"\n",
    "\n",
    "DATASET_NAME = EXP_NAME\n",
    "OUTPUT_PATH = f\"output/retriever/{EXP_NAME}\"\n",
    "# MODEL_OUTPUT_PATH = f\"{OUTPUT_PATH}/trained_model\"\n",
    "MODEL_OUTPUT_PATH = f\"{OUTPUT_PATH}/checkpoint-5010\"\n",
    "\n",
    "TRAIN_USE_NUM = 5\n",
    "RETRIEVE_NUM = 25  # TODO: 多くしてみる\n",
    "\n",
    "EPOCH = 5\n",
    "BS = 1  # 2  # 4  # 8\n",
    "GRAD_ACC_STEP = 1  # bugがあるので1にする。# 128 // BS\n",
    "LR = 2e-5\n",
    "\n",
    "TRAINING = True\n",
    "DEBUG = False\n",
    "WANDB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30d2d7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:23.119381Z",
     "iopub.status.busy": "2024-11-04T08:57:23.118824Z",
     "iopub.status.idle": "2024-11-04T08:57:23.129894Z",
     "shell.execute_reply": "2024-11-04T08:57:23.129320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/eedi-mining-misconceptions-in-mathematics/exp/retriever\n",
      "VastAi! Retriever\n",
      "../../data\n",
      "/root/eedi-mining-misconceptions-in-mathematics/exp/retriever\n",
      "VastAi! Retriever\n",
      "../../output/retriever/e016-ret-stella\n",
      "/root/eedi-mining-misconceptions-in-mathematics/exp/retriever\n",
      "VastAi! Retriever\n",
      "../../output/retriever/e016-ret-stella/checkpoint-5010\n",
      "/root/eedi-mining-misconceptions-in-mathematics/exp/retriever\n",
      "VastAi! Retriever\n",
      "../../env_file\n",
      "/root/eedi-mining-misconceptions-in-mathematics/exp/retriever\n",
      "VastAi! Retriever\n",
      "../../output/retriever/e003-ret-bge/e003-ret-bge-ret25-map0.1841-recall0.5506.csv\n"
     ]
    }
   ],
   "source": [
    "def resolve_path(base_path: str) -> str:\n",
    "    import os\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    if cwd == f\"/notebooks\":\n",
    "        print(\"Jupyter Kernel By VSCode!\")\n",
    "        return f\"/notebooks/{COMPETITION_NAME}/{base_path}\"\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}\":\n",
    "        print(\"nohup!\")\n",
    "        return base_path\n",
    "    elif cwd == f\"/notebooks/{COMPETITION_NAME}/{COMPETITION_NAME}/exp\":\n",
    "        print(\"Jupyter Lab!\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd == f\"/root/{COMPETITION_NAME}/exp/reranker\":\n",
    "        print(\"VastAi! Reranker\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd == f\"/root/{COMPETITION_NAME}/exp/retriever\":\n",
    "        print(\"VastAi! Retriever\")\n",
    "        return f\"../../{base_path}\"\n",
    "    elif cwd == f\"/root/{COMPETITION_NAME}\":\n",
    "        print(\"VastAi!\")\n",
    "        return base_path\n",
    "    else:\n",
    "        raise Exception(\"Unknown environment\")\n",
    "\n",
    "\n",
    "DATA_PATH = resolve_path(DATA_PATH)\n",
    "print(DATA_PATH)\n",
    "OUTPUT_PATH = resolve_path(OUTPUT_PATH)\n",
    "print(OUTPUT_PATH)\n",
    "MODEL_OUTPUT_PATH = resolve_path(MODEL_OUTPUT_PATH)\n",
    "print(MODEL_OUTPUT_PATH)\n",
    "ENV_PATH = resolve_path(ENV_PATH)\n",
    "print(ENV_PATH)\n",
    "RETRIVED_FILE_PATH = resolve_path(RETRIVED_FILE_PATH)\n",
    "print(RETRIVED_FILE_PATH)\n",
    "# RETRIVED_EX_FILE_PATH = resolve_path(RETRIVED_EX_FILE_PATH)\n",
    "# print(RETRIVED_EX_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e26266",
   "metadata": {
    "papermill": {
     "duration": 0.010544,
     "end_time": "2024-09-14T01:45:57.095623",
     "exception": false,
     "start_time": "2024-09-14T01:45:57.085079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5c0dbcc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:23.132281Z",
     "iopub.status.busy": "2024-11-04T08:57:23.131935Z",
     "iopub.status.idle": "2024-11-04T08:57:24.913518Z",
     "shell.execute_reply": "2024-11-04T08:57:24.913150Z"
    },
    "papermill": {
     "duration": 2.430721,
     "end_time": "2024-09-14T01:45:59.537229",
     "exception": false,
     "start_time": "2024-09-14T01:45:57.106508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import wandb\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    default_data_collator,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc40cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:24.915078Z",
     "iopub.status.busy": "2024-11-04T08:57:24.914894Z",
     "iopub.status.idle": "2024-11-04T08:57:24.916834Z",
     "shell.execute_reply": "2024-11-04T08:57:24.916610Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "# import sentence_transformers\n",
    "\n",
    "assert transformers.__version__ == \"4.44.2\"\n",
    "# assert sentence_transformers.__version__ == \"3.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d1b181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:24.917981Z",
     "iopub.status.busy": "2024-11-04T08:57:24.917887Z",
     "iopub.status.idle": "2024-11-04T08:57:24.919556Z",
     "shell.execute_reply": "2024-11-04T08:57:24.919335Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_PROC = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85aefdc",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896ee0be",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:24.920709Z",
     "iopub.status.busy": "2024-11-04T08:57:24.920602Z",
     "iopub.status.idle": "2024-11-04T08:57:24.925768Z",
     "shell.execute_reply": "2024-11-04T08:57:24.925548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(f\"{ENV_PATH}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb645be4",
   "metadata": {},
   "source": [
    "# WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7360a36",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:24.926995Z",
     "iopub.status.busy": "2024-11-04T08:57:24.926863Z",
     "iopub.status.idle": "2024-11-04T08:57:25.964125Z",
     "shell.execute_reply": "2024-11-04T08:57:25.963801Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msinchir0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/eedi-mining-misconceptions-in-mathematics/exp/retriever/wandb/run-20241109_080804-pbc1pttz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics/runs/pbc1pttz' target=\"_blank\">e016-ret-stella</a></strong> to <a href='https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics' target=\"_blank\">https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics/runs/pbc1pttz' target=\"_blank\">https://wandb.ai/sinchir0/eedi-mining-misconceptions-in-mathematics/runs/pbc1pttz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'wandb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if WANDB:\n",
    "    wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
    "    wandb.init(project=COMPETITION_NAME, name=EXP_NAME)\n",
    "    REPORT_TO = \"wandb\"\n",
    "else:\n",
    "    REPORT_TO = \"none\"\n",
    "\n",
    "REPORT_TO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98570c57",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7320b8",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:25.965216Z",
     "iopub.status.busy": "2024-11-04T08:57:25.965127Z",
     "iopub.status.idle": "2024-11-04T08:57:27.279128Z",
     "shell.execute_reply": "2024-11-04T08:57:27.278560Z"
    }
   },
   "outputs": [],
   "source": [
    "train = (\n",
    "    load_dataset(\n",
    "        \"csv\",\n",
    "        data_files=RETRIVED_FILE_PATH,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    .filter(\n",
    "        lambda example: example[\"MisconceptionId\"] is not None,\n",
    "        num_proc=NUM_PROC,\n",
    "    )\n",
    "    .filter(  # anchor、positive、negativeの構成にするため、positiveとnegativeが一致している行を削除する\n",
    "        lambda example: example[\"MisconceptionId\"] != example[\"PredictMisconceptionId\"],\n",
    "        num_proc=NUM_PROC,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626b8229",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:27.281007Z",
     "iopub.status.busy": "2024-11-04T08:57:27.280856Z",
     "iopub.status.idle": "2024-11-04T08:57:27.283736Z",
     "shell.execute_reply": "2024-11-04T08:57:27.283517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['QuestionId', 'ConstructName', 'SubjectName', 'QuestionText', 'CorrectAnswer', 'AnswerType', 'AnswerText', 'AllText', 'AnswerAlphabet', 'QuestionId_Answer', 'MisconceptionId', 'PredictMisconceptionId', 'target', 'MisconceptionName', 'PredictMisconceptionName'],\n",
       "    num_rows: 106844\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a95079c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:27.284995Z",
     "iopub.status.busy": "2024-11-04T08:57:27.284881Z",
     "iopub.status.idle": "2024-11-04T08:57:27.286441Z",
     "shell.execute_reply": "2024-11-04T08:57:27.286218Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 同じMisconceptionNameのpositiveの例を25個→5個に減らす\n",
    "# train = Dataset.from_pandas(\n",
    "#     train.to_pandas().groupby(\"QuestionId_Answer\").head(TRAIN_USE_NUM)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc872920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:27.287717Z",
     "iopub.status.busy": "2024-11-04T08:57:27.287578Z",
     "iopub.status.idle": "2024-11-04T08:57:27.289208Z",
     "shell.execute_reply": "2024-11-04T08:57:27.288956Z"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train = train.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fafd4a3",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:27.290450Z",
     "iopub.status.busy": "2024-11-04T08:57:27.290293Z",
     "iopub.status.idle": "2024-11-04T08:57:27.595637Z",
     "shell.execute_reply": "2024-11-04T08:57:27.595127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the order of operations to carry out calculations involving powers BIDMAS \\[\n",
      "3 \\times 2+4-5\n",
      "\\]\n",
      "Where do the brackets need to go to make the answer equal \\( 13 \\) ? Does not need brackets\n"
     ]
    }
   ],
   "source": [
    "print(train[\"AllText\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c762b2",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:27.596812Z",
     "iopub.status.busy": "2024-11-04T08:57:27.596715Z",
     "iopub.status.idle": "2024-11-04T08:57:27.869902Z",
     "shell.execute_reply": "2024-11-04T08:57:27.869517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confuses the order of operations, believes addition comes before multiplication \n"
     ]
    }
   ],
   "source": [
    "print(train[\"MisconceptionName\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9db6787d",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:27.871029Z",
     "iopub.status.busy": "2024-11-04T08:57:27.870921Z",
     "iopub.status.idle": "2024-11-04T08:57:28.069884Z",
     "shell.execute_reply": "2024-11-04T08:57:28.069614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applies BIDMAS in strict order (does not realize addition and subtraction, and multiplication and division, are of equal priority)\n"
     ]
    }
   ],
   "source": [
    "print(train[\"PredictMisconceptionName\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89161999",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:28.071214Z",
     "iopub.status.busy": "2024-11-04T08:57:28.071079Z",
     "iopub.status.idle": "2024-11-04T08:57:28.072827Z",
     "shell.execute_reply": "2024-11-04T08:57:28.072604Z"
    }
   },
   "outputs": [],
   "source": [
    "# # load ex data\n",
    "# train_ex = (\n",
    "#     load_dataset(\n",
    "#         \"csv\",\n",
    "#         data_files=RETRIVED_EX_FILE_PATH,\n",
    "#         split=\"train\",\n",
    "#     )\n",
    "#     .filter(  # Nameを正しく生成できず、Idが紐づかなかったデータを落とす\n",
    "#         lambda example: example[\"MisconceptionId\"] is not None,\n",
    "#         num_proc=NUM_PROC,\n",
    "#     )\n",
    "#     .filter(  # anchor、positive、negativeの構成にするため、positiveとnegativeが一致している行を削除する\n",
    "#         lambda example: example[\"MisconceptionId\"] != example[\"PredictMisconceptionId\"],\n",
    "#         num_proc=NUM_PROC,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# train_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf26818",
   "metadata": {},
   "source": [
    "# Train Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dcb4968",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:28.074123Z",
     "iopub.status.busy": "2024-11-04T08:57:28.073924Z",
     "iopub.status.idle": "2024-11-04T08:57:29.189991Z",
     "shell.execute_reply": "2024-11-04T08:57:29.189296Z"
    }
   },
   "outputs": [],
   "source": [
    "train, valid = (\n",
    "    train.filter(lambda example: example[\"QuestionId\"] % 3 != 0, num_proc=NUM_PROC),\n",
    "    train.filter(lambda example: example[\"QuestionId\"] % 3 == 0, num_proc=NUM_PROC),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8184ebdf",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:29.192168Z",
     "iopub.status.busy": "2024-11-04T08:57:29.192033Z",
     "iopub.status.idle": "2024-11-04T08:57:29.195734Z",
     "shell.execute_reply": "2024-11-04T08:57:29.195088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['QuestionId', 'ConstructName', 'SubjectName', 'QuestionText', 'CorrectAnswer', 'AnswerType', 'AnswerText', 'AllText', 'AnswerAlphabet', 'QuestionId_Answer', 'MisconceptionId', 'PredictMisconceptionId', 'target', 'MisconceptionName', 'PredictMisconceptionName'],\n",
      "    num_rows: 71119\n",
      "})\n",
      "Dataset({\n",
      "    features: ['QuestionId', 'ConstructName', 'SubjectName', 'QuestionText', 'CorrectAnswer', 'AnswerType', 'AnswerText', 'AllText', 'AnswerAlphabet', 'QuestionId_Answer', 'MisconceptionId', 'PredictMisconceptionId', 'target', 'MisconceptionName', 'PredictMisconceptionName'],\n",
      "    num_rows: 35725\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "print(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0eeff4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:29.197693Z",
     "iopub.status.busy": "2024-11-04T08:57:29.197466Z",
     "iopub.status.idle": "2024-11-04T08:57:29.199897Z",
     "shell.execute_reply": "2024-11-04T08:57:29.199424Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import concatenate_datasets\n",
    "\n",
    "# train = concatenate_datasets([train, train_ex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18dcd2b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:29.202218Z",
     "iopub.status.busy": "2024-11-04T08:57:29.201951Z",
     "iopub.status.idle": "2024-11-04T08:57:29.204416Z",
     "shell.execute_reply": "2024-11-04T08:57:29.203866Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(train)\n",
    "# print(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d11def",
   "metadata": {
    "papermill": {
     "duration": 0.010829,
     "end_time": "2024-09-14T01:46:00.109663",
     "exception": false,
     "start_time": "2024-09-14T01:46:00.098834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3888fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoModelForSentenceEmbedding(nn.Module):\n",
    "    def __init__(self, model_name, tokenizer, normalize=True):\n",
    "        super(AutoModelForSentenceEmbedding, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name\n",
    "        )  # , quantizaton_config=BitsAndBytesConfig(load_in_8bit=True), device_map={\"\":0})\n",
    "        self.normalize = normalize\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        model_output = self.model(**kwargs)\n",
    "        embeddings = self.mean_pooling(model_output, kwargs[\"attention_mask\"])\n",
    "        if self.normalize:\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[\n",
    "            0\n",
    "        ]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        )\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "            input_mask_expanded.sum(1), min=1e-9\n",
    "        )\n",
    "\n",
    "    def __getattr__(self, name: str):\n",
    "        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n",
    "        try:\n",
    "            return super().__getattr__(name)  # defer to nn.Module's logic\n",
    "        except AttributeError:\n",
    "            return getattr(self.model, name)\n",
    "\n",
    "\n",
    "def get_cosing_embeddings(query_embs, product_embs):\n",
    "    return torch.sum(query_embs * product_embs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a0e7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "816e0be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model\n",
    "model = AutoModelForSentenceEmbedding(MODEL_NAME, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f234cab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoModelForSentenceEmbedding(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151646, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b19070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,232,384 || all params: 1,552,501,248 || trainable%: 0.5947\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36dc8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator_kwargs = {\n",
    "    \"gradient_accumulation_steps\": GRAD_ACC_STEP,\n",
    "    \"log_with\": REPORT_TO,\n",
    "    \"project_dir\": OUTPUT_PATH,\n",
    "}\n",
    "\n",
    "accelerator = Accelerator(**accelerator_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d727b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): AutoModelForSentenceEmbedding(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151646, 1536)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2SdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): Qwen2RotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8960, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "accelerator.print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0dd8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator, get_scheduler\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=BS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    valid,\n",
    "    shuffle=False,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=BS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine_with_restarts\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.1,\n",
    "    num_training_steps=EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb00e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e066537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fd7281434f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_model_hook(models, weights, output_dir):\n",
    "    for i, model in enumerate(models):\n",
    "        model.save_pretrained(output_dir, state_dict=weights[i])\n",
    "        # make sure to pop weight so that corresponding model is not saved again\n",
    "        weights.pop()\n",
    "\n",
    "\n",
    "def load_model_hook(models, input_dir):\n",
    "    while len(models) > 0:\n",
    "        model = models.pop()\n",
    "        # pop models so that they are not loaded again\n",
    "        if hasattr(model, \"active_adapter\") and hasattr(model, \"load_adapter\"):\n",
    "            model.load_adapter(input_dir, model.active_adapter, is_trainable=True)\n",
    "\n",
    "\n",
    "# saving and loading checkpoints for resuming training\n",
    "accelerator.register_save_state_pre_hook(save_model_hook)\n",
    "accelerator.register_load_state_pre_hook(load_model_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35b89cc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m---> 15\u001b[0m         query_embs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m         product_embs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m     20\u001b[0m                 k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m): v\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m             }\n\u001b[1;32m     24\u001b[0m         )\n\u001b[1;32m     25\u001b[0m         loss \u001b[38;5;241m=\u001b[39m get_loss(\n\u001b[1;32m     26\u001b[0m             get_cosing_embeddings(query_embs, product_embs), batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:2568\u001b[0m, in \u001b[0;36mPeftModelForFeatureExtraction.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   2566\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2567\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 2568\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2573\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2575\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2578\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   2579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2580\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m, in \u001b[0;36mAutoModelForSentenceEmbedding.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 12\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_pooling(model_output, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:857\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    854\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m^\u001b[39m (inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 857\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m     )\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one"
     ]
    }
   ],
   "source": [
    "def get_loss(cosine_score, labels):\n",
    "    return torch.mean(\n",
    "        torch.square(\n",
    "            labels * (1 - cosine_score)\n",
    "            + torch.clamp((1 - labels) * cosine_score, min=0.0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "total_loss = 0\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(model):\n",
    "            query_embs = model(\n",
    "                **{k.replace(\"query_\", \"\"): v for k, v in batch.items() if \"query\" in k}\n",
    "            )\n",
    "            product_embs = model(\n",
    "                **{\n",
    "                    k.replace(\"product_\", \"\"): v\n",
    "                    for k, v in batch.items()\n",
    "                    if \"product\" in k\n",
    "                }\n",
    "            )\n",
    "            loss = get_loss(\n",
    "                get_cosing_embeddings(query_embs, product_embs), batch[\"labels\"]\n",
    "            )\n",
    "            total_loss += accelerator.reduce(loss.detach().float(), reduction=\"sum\")\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89fc80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QuestionId': tensor([908], device='cuda:0'),\n",
       " 'MisconceptionId': tensor([1691], device='cuda:0'),\n",
       " 'PredictMisconceptionId': tensor([1810], device='cuda:0'),\n",
       " 'target': tensor([0], device='cuda:0')}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: ここから再開する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec1e23",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-11-04T08:57:29.206302Z",
     "iopub.status.busy": "2024-11-04T08:57:29.206084Z",
     "iopub.status.idle": "2024-11-04T09:04:58.765838Z",
     "shell.execute_reply": "2024-11-04T09:04:58.765234Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Qwen2Model' object has no attribute 'tokenize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:787\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PeftModelForFeatureExtraction' object has no attribute 'tokenize'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:360\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LoraModel' object has no attribute 'tokenize'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 33\u001b[0m, in \u001b[0;36mAutoModelForSentenceEmbedding.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AutoModelForSentenceEmbedding' object has no attribute 'tokenize'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 43\u001b[0m\n\u001b[1;32m     35\u001b[0m dev_evaluator \u001b[38;5;241m=\u001b[39m TripletEvaluator(\n\u001b[1;32m     36\u001b[0m     anchors\u001b[38;5;241m=\u001b[39mvalid[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllText\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     37\u001b[0m     positives\u001b[38;5;241m=\u001b[39mvalid[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMisconceptionName\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     38\u001b[0m     negatives\u001b[38;5;241m=\u001b[39mvalid[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictMisconceptionName\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     39\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-dev\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# dev_evaluator(model)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformerTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAllText\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMisconceptionName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPredictMisconceptionName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAllText\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMisconceptionName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPredictMisconceptionName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     57\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(MODEL_OUTPUT_PATH)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/trainer.py:180\u001b[0m, in \u001b[0;36mSentenceTransformerTrainer.__init__\u001b[0;34m(self, model, args, train_dataset, eval_dataset, loss, evaluator, data_collator, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    177\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_collator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m SentenceTransformerDataCollator(tokenize_fn\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m], [train_dataset, eval_dataset]):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, IterableDataset) \u001b[38;5;129;01mand\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcolumn_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:791\u001b[0m, in \u001b[0;36mPeftModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# see #1892: prevent infinite recursion if class is not initialized\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/model.py:364\u001b[0m, in \u001b[0;36mLoraModel.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# see #1892: prevent infinite recursion if class is not initialized\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 35\u001b[0m, in \u001b[0;36mAutoModelForSentenceEmbedding.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name)  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Qwen2Model' object has no attribute 'tokenize'"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    model = SentenceTransformer(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "    loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    args = SentenceTransformerTrainingArguments(\n",
    "        # Required parameter:\n",
    "        output_dir=OUTPUT_PATH,\n",
    "        # Optional training parameters:\n",
    "        num_train_epochs=EPOCH,\n",
    "        per_device_train_batch_size=BS,  # 16,\n",
    "        gradient_accumulation_steps=GRAD_ACC_STEP,\n",
    "        per_device_eval_batch_size=BS,  # 16,\n",
    "        eval_accumulation_steps=GRAD_ACC_STEP,\n",
    "        learning_rate=LR,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "        fp16_full_eval=True,\n",
    "        bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "        batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "        # Optional tracking/debugging parameters:\n",
    "        lr_scheduler_type=\"cosine_with_restarts\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=0.1,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=0.1,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=100,\n",
    "        report_to=REPORT_TO,  # Will be used in W&B if `wandb` is installed\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "\n",
    "    dev_evaluator = TripletEvaluator(\n",
    "        anchors=valid[\"AllText\"],\n",
    "        positives=valid[\"MisconceptionName\"],\n",
    "        negatives=valid[\"PredictMisconceptionName\"],\n",
    "        name=f\"{MODEL_NAME}-dev\",\n",
    "    )\n",
    "    # dev_evaluator(model)\n",
    "\n",
    "    trainer = SentenceTransformerTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train.select_columns(\n",
    "            [\"AllText\", \"MisconceptionName\", \"PredictMisconceptionName\"]\n",
    "        ),\n",
    "        eval_dataset=valid.select_columns(\n",
    "            [\"AllText\", \"MisconceptionName\", \"PredictMisconceptionName\"]\n",
    "        ),\n",
    "        loss=loss,\n",
    "        evaluator=dev_evaluator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(MODEL_OUTPUT_PATH)\n",
    "else:\n",
    "    model = SentenceTransformer(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5e067",
   "metadata": {},
   "source": [
    "# Make Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e617cb",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:55.202611Z",
     "iopub.status.busy": "2024-10-13T17:53:55.202065Z",
     "iopub.status.idle": "2024-10-13T17:53:55.229337Z",
     "shell.execute_reply": "2024-10-13T17:53:55.228568Z"
    }
   },
   "outputs": [],
   "source": [
    "common_col = [\n",
    "    \"QuestionId\",\n",
    "    \"ConstructName\",\n",
    "    \"SubjectName\",\n",
    "    \"QuestionText\",\n",
    "    \"CorrectAnswer\",\n",
    "]\n",
    "\n",
    "train_long = (\n",
    "    pl.read_csv(f\"{DATA_PATH}/train.csv\")\n",
    "    .select(\n",
    "        pl.col(common_col + [f\"Answer{alpha}Text\" for alpha in [\"A\", \"B\", \"C\", \"D\"]])\n",
    "    )\n",
    "    .unpivot(\n",
    "        index=common_col,\n",
    "        variable_name=\"AnswerType\",\n",
    "        value_name=\"AnswerText\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col(\"ConstructName\"),\n",
    "                pl.col(\"SubjectName\"),\n",
    "                pl.col(\"QuestionText\"),\n",
    "                pl.col(\"AnswerText\"),\n",
    "            ],\n",
    "            separator=\" \",\n",
    "        ).alias(\"AllText\"),\n",
    "        pl.col(\"AnswerType\").str.extract(r\"Answer([A-D])Text$\").alias(\"AnswerAlphabet\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.concat_str(\n",
    "            [pl.col(\"QuestionId\"), pl.col(\"AnswerAlphabet\")], separator=\"_\"\n",
    "        ).alias(\"QuestionId_Answer\"),\n",
    "    )\n",
    "    .sort(\"QuestionId_Answer\")\n",
    ")\n",
    "train_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee44be4",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:55.233046Z",
     "iopub.status.busy": "2024-10-13T17:53:55.232751Z",
     "iopub.status.idle": "2024-10-13T17:53:55.247350Z",
     "shell.execute_reply": "2024-10-13T17:53:55.246649Z"
    }
   },
   "outputs": [],
   "source": [
    "train_misconception_long = (\n",
    "    pl.read_csv(f\"{DATA_PATH}/train.csv\")\n",
    "    .select(\n",
    "        pl.col(\n",
    "            common_col + [f\"Misconception{alpha}Id\" for alpha in [\"A\", \"B\", \"C\", \"D\"]]\n",
    "        )\n",
    "    )\n",
    "    .unpivot(\n",
    "        index=common_col,\n",
    "        variable_name=\"MisconceptionType\",\n",
    "        value_name=\"MisconceptionId\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"MisconceptionType\")\n",
    "        .str.extract(r\"Misconception([A-D])Id$\")\n",
    "        .alias(\"AnswerAlphabet\"),\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.concat_str(\n",
    "            [pl.col(\"QuestionId\"), pl.col(\"AnswerAlphabet\")], separator=\"_\"\n",
    "        ).alias(\"QuestionId_Answer\"),\n",
    "    )\n",
    "    .sort(\"QuestionId_Answer\")\n",
    "    .select(pl.col([\"QuestionId_Answer\", \"MisconceptionId\"]))\n",
    "    .with_columns(pl.col(\"MisconceptionId\").cast(pl.Int64))\n",
    ")\n",
    "train_misconception_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d824b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:55.250433Z",
     "iopub.status.busy": "2024-10-13T17:53:55.250274Z",
     "iopub.status.idle": "2024-10-13T17:53:55.254341Z",
     "shell.execute_reply": "2024-10-13T17:53:55.253650Z"
    }
   },
   "outputs": [],
   "source": [
    "# join MisconceptionId\n",
    "train_long = train_long.join(train_misconception_long, on=\"QuestionId_Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9bad18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:55.258006Z",
     "iopub.status.busy": "2024-10-13T17:53:55.257661Z",
     "iopub.status.idle": "2024-10-13T17:53:55.405542Z",
     "shell.execute_reply": "2024-10-13T17:53:55.404616Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_long = train_long.filter(\n",
    "    pl.col(\"QuestionId_Answer\").is_in(set(valid[\"QuestionId_Answer\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22563ddd",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:55.409889Z",
     "iopub.status.busy": "2024-10-13T17:53:55.409275Z",
     "iopub.status.idle": "2024-10-13T17:53:55.415820Z",
     "shell.execute_reply": "2024-10-13T17:53:55.415125Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d8541",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:55.419988Z",
     "iopub.status.busy": "2024-10-13T17:53:55.419632Z",
     "iopub.status.idle": "2024-10-13T17:53:59.533832Z",
     "shell.execute_reply": "2024-10-13T17:53:59.532612Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = SentenceTransformer(MODEL_OUTPUT_PATH)\n",
    "\n",
    "valid_long_vec = model.encode(\n",
    "    valid_long[\"AllText\"].to_list(), normalize_embeddings=True\n",
    ")\n",
    "misconception_mapping = pl.read_csv(f\"{DATA_PATH}/misconception_mapping.csv\")\n",
    "misconception_mapping_vec = model.encode(\n",
    "    misconception_mapping[\"MisconceptionName\"].to_list(), normalize_embeddings=True\n",
    ")\n",
    "print(valid_long_vec.shape)\n",
    "print(misconception_mapping_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368fe01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:59.537606Z",
     "iopub.status.busy": "2024-10-13T17:53:59.537376Z",
     "iopub.status.idle": "2024-10-13T17:53:59.546717Z",
     "shell.execute_reply": "2024-10-13T17:53:59.545898Z"
    }
   },
   "outputs": [],
   "source": [
    "# misconception_mapping_vecを保存する\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "np.save(f\"{OUTPUT_PATH}/misconception_mapping_vec.npy\", misconception_mapping_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171be61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:59.550365Z",
     "iopub.status.busy": "2024-10-13T17:53:59.549981Z",
     "iopub.status.idle": "2024-10-13T17:53:59.979226Z",
     "shell.execute_reply": "2024-10-13T17:53:59.978348Z"
    },
    "papermill": {
     "duration": 5.023799,
     "end_time": "2024-09-14T01:46:06.422421",
     "exception": false,
     "start_time": "2024-09-14T01:46:01.398622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_cos_sim_arr = cosine_similarity(valid_long_vec, misconception_mapping_vec)\n",
    "valid_sorted_indices = np.argsort(-valid_cos_sim_arr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288a590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:59.983340Z",
     "iopub.status.busy": "2024-10-13T17:53:59.983050Z",
     "iopub.status.idle": "2024-10-13T17:53:59.987232Z",
     "shell.execute_reply": "2024-10-13T17:53:59.986637Z"
    },
    "papermill": {
     "duration": 0.025202,
     "end_time": "2024-09-14T01:46:06.496371",
     "exception": false,
     "start_time": "2024-09-14T01:46:06.471169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example\n",
    "def print_example(df: pl.DataFrame, sorted_indices: np.ndarray, idx: int) -> None:\n",
    "    print(f\"Query idx{idx}\")\n",
    "    print(df[\"AllText\"][idx])\n",
    "    print(\"\\nCos Sim No.1\")\n",
    "    print(misconception_mapping[\"MisconceptionName\"][int(sorted_indices[idx, 0])])\n",
    "    print(\"\\nCos Sim No.2\")\n",
    "    print(misconception_mapping[\"MisconceptionName\"][int(sorted_indices[idx, 1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85607c",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:59.990711Z",
     "iopub.status.busy": "2024-10-13T17:53:59.990388Z",
     "iopub.status.idle": "2024-10-13T17:53:59.994817Z",
     "shell.execute_reply": "2024-10-13T17:53:59.994217Z"
    }
   },
   "outputs": [],
   "source": [
    "print_example(train_long, valid_sorted_indices, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211ff44",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T17:53:59.998285Z",
     "iopub.status.busy": "2024-10-13T17:53:59.997924Z",
     "iopub.status.idle": "2024-10-13T17:54:00.001908Z",
     "shell.execute_reply": "2024-10-13T17:54:00.001312Z"
    },
    "papermill": {
     "duration": 0.022384,
     "end_time": "2024-09-14T01:46:06.530441",
     "exception": false,
     "start_time": "2024-09-14T01:46:06.508057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_example(train_long, valid_sorted_indices, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe1e2c",
   "metadata": {
    "papermill": {
     "duration": 0.011086,
     "end_time": "2024-09-14T01:46:06.552908",
     "exception": false,
     "start_time": "2024-09-14T01:46:06.541822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b5f922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:54:00.005248Z",
     "iopub.status.busy": "2024-10-13T17:54:00.004916Z",
     "iopub.status.idle": "2024-10-13T17:54:00.024067Z",
     "shell.execute_reply": "2024-10-13T17:54:00.023456Z"
    },
    "papermill": {
     "duration": 0.04065,
     "end_time": "2024-09-14T01:46:06.605893",
     "exception": false,
     "start_time": "2024-09-14T01:46:06.565243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_long = valid_long.with_columns(\n",
    "    pl.Series(valid_sorted_indices[:, :RETRIEVE_NUM].tolist()).alias(\n",
    "        \"PredictMisconceptionId\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b400a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:54:00.027577Z",
     "iopub.status.busy": "2024-10-13T17:54:00.027212Z",
     "iopub.status.idle": "2024-10-13T17:54:00.030942Z",
     "shell.execute_reply": "2024-10-13T17:54:00.030350Z"
    },
    "papermill": {
     "duration": 0.023321,
     "end_time": "2024-09-14T01:46:06.640719",
     "exception": false,
     "start_time": "2024-09-14T01:46:06.617398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-1#MAP@3-Metric\n",
    "def map_at_25(predictions, labels):\n",
    "    map_sum = 0\n",
    "    for x, y in zip(predictions, labels):\n",
    "        z = [1 / i if y == j else 0 for i, j in zip(range(1, 26), x)]\n",
    "        map_sum += np.sum(z)\n",
    "    return map_sum / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd99f42",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T17:54:00.034316Z",
     "iopub.status.busy": "2024-10-13T17:54:00.034034Z",
     "iopub.status.idle": "2024-10-13T17:54:00.063176Z",
     "shell.execute_reply": "2024-10-13T17:54:00.062601Z"
    },
    "papermill": {
     "duration": 0.113621,
     "end_time": "2024-09-14T01:46:06.765721",
     "exception": false,
     "start_time": "2024-09-14T01:46:06.652100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_at_25_score = map_at_25(\n",
    "    valid_long.filter(pl.col(\"MisconceptionId\").is_not_null())[\n",
    "        \"PredictMisconceptionId\"\n",
    "    ],\n",
    "    valid_long.filter(pl.col(\"MisconceptionId\").is_not_null())[\"MisconceptionId\"],\n",
    ")\n",
    "map_at_25_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a66afa0",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T17:54:00.066541Z",
     "iopub.status.busy": "2024-10-13T17:54:00.066180Z",
     "iopub.status.idle": "2024-10-13T17:54:00.217272Z",
     "shell.execute_reply": "2024-10-13T17:54:00.216574Z"
    },
    "papermill": {
     "duration": 0.034606,
     "end_time": "2024-09-14T01:46:06.812192",
     "exception": false,
     "start_time": "2024-09-14T01:46:06.777586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall(predictions, labels):\n",
    "    acc_num = np.sum([1 for x, y in zip(predictions, labels) if y in x])\n",
    "    return acc_num / len(predictions)\n",
    "\n",
    "\n",
    "recall_score = recall(\n",
    "    valid_long.filter(pl.col(\"MisconceptionId\").is_not_null())[\n",
    "        \"PredictMisconceptionId\"\n",
    "    ],\n",
    "    valid_long.filter(pl.col(\"MisconceptionId\").is_not_null())[\"MisconceptionId\"],\n",
    ")\n",
    "recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0988734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:54:00.220749Z",
     "iopub.status.busy": "2024-10-13T17:54:00.220376Z",
     "iopub.status.idle": "2024-10-13T17:54:00.223789Z",
     "shell.execute_reply": "2024-10-13T17:54:00.223094Z"
    }
   },
   "outputs": [],
   "source": [
    "# output_textを保存\n",
    "with open(f\"{MODEL_OUTPUT_PATH}/cv_score.txt\", \"w\") as f:\n",
    "    f.write(f\"MAP@25:{map_at_25_score:.4f}, Recall:{recall_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657bf3d",
   "metadata": {
    "papermill": {
     "duration": 0.011875,
     "end_time": "2024-09-14T01:46:07.146569",
     "exception": false,
     "start_time": "2024-09-14T01:46:07.134694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make Retrieved Train File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ca4b1",
   "metadata": {
    "autoscroll": "auto",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-13T17:54:00.227064Z",
     "iopub.status.busy": "2024-10-13T17:54:00.226744Z",
     "iopub.status.idle": "2024-10-13T17:54:00.237168Z",
     "shell.execute_reply": "2024-10-13T17:54:00.236462Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_retrieved = (\n",
    "    valid_long.filter(pl.col(\"MisconceptionId\").is_not_null())\n",
    "    .explode(\"PredictMisconceptionId\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"MisconceptionId\") == pl.col(\"PredictMisconceptionId\"))\n",
    "        .cast(pl.Int64)\n",
    "        .alias(\"target\")\n",
    "    )\n",
    "    .join(\n",
    "        misconception_mapping.with_columns(pl.all().name.prefix(\"Predict\")),\n",
    "        on=\"PredictMisconceptionId\",\n",
    "    )\n",
    ")\n",
    "valid_retrieved.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd7ac6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:54:00.240890Z",
     "iopub.status.busy": "2024-10-13T17:54:00.240326Z",
     "iopub.status.idle": "2024-10-13T17:54:00.262855Z",
     "shell.execute_reply": "2024-10-13T17:54:00.261970Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_retrieved.write_csv(\n",
    "    f\"{MODEL_OUTPUT_PATH}/{EXP_NAME}-valid-ret{RETRIEVE_NUM}-map{map_at_25_score:.4f}-recall{recall_score:.4f}.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d6dcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:54:00.266236Z",
     "iopub.status.busy": "2024-10-13T17:54:00.265939Z",
     "iopub.status.idle": "2024-10-13T17:54:00.270059Z",
     "shell.execute_reply": "2024-10-13T17:54:00.269461Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"{MODEL_OUTPUT_PATH}/{EXP_NAME}-valid-ret{RETRIEVE_NUM}-map{map_at_25_score:.4f}-recall{recall_score:.4f}.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa7596",
   "metadata": {},
   "source": [
    "# Kaggle Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368627a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T17:54:00.273197Z",
     "iopub.status.busy": "2024-10-13T17:54:00.273039Z",
     "iopub.status.idle": "2024-10-13T17:58:23.850447Z",
     "shell.execute_reply": "2024-10-13T17:58:23.848332Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "\n",
    "def dataset_create_new(dataset_name: str, upload_dir: str):\n",
    "    # if \"_\" in dataset_name:\n",
    "    #     raise ValueError(\"datasetの名称に_の使用は禁止です\")\n",
    "    dataset_metadata = {}\n",
    "    dataset_metadata[\"id\"] = f\"sinchir0/{dataset_name}\"\n",
    "    dataset_metadata[\"licenses\"] = [{\"name\": \"CC0-1.0\"}]\n",
    "    dataset_metadata[\"title\"] = dataset_name\n",
    "    with open(os.path.join(upload_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode=\"tar\")\n",
    "\n",
    "\n",
    "print(f\"Create Dataset name:{DATASET_NAME}, output_dir:{OUTPUT_PATH}\")\n",
    "dataset_create_new(dataset_name=DATASET_NAME, upload_dir=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20375e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9551816,
     "sourceId": 82695,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.614286,
   "end_time": "2024-09-14T01:46:08.075104",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-14T01:45:53.460818",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "25da2f78e964412589f99a1e4a645b67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "264822a703204dfc89d15253da32e2a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "32b1e04bd8604568a745f6cb757cb4ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_be53d6809fd14c1299091d250745d4d4",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5133d294d91f48909fa4c24e7df7f1bc",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "5133d294d91f48909fa4c24e7df7f1bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "94da6364db30452f8b4991c203d8c94d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_99b6036e0c424a998c600443415bd61c",
        "IPY_MODEL_32b1e04bd8604568a745f6cb757cb4ca",
        "IPY_MODEL_9cec1df457744eed8dac667f438ee390"
       ],
       "layout": "IPY_MODEL_264822a703204dfc89d15253da32e2a2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "99b6036e0c424a998c600443415bd61c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b7660448dc514841b062bd08d8f0233c",
       "placeholder": "​",
       "style": "IPY_MODEL_cd9fd81e133c4775bb6f66b326ecb6d7",
       "tabbable": null,
       "tooltip": null,
       "value": "Computing widget examples:   0%"
      }
     },
     "9cec1df457744eed8dac667f438ee390": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_be3c8efcb7e54c84b5244c1763a4bf06",
       "placeholder": "​",
       "style": "IPY_MODEL_25da2f78e964412589f99a1e4a645b67",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/1 [00:00&lt;?, ?example/s]"
      }
     },
     "b7660448dc514841b062bd08d8f0233c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be3c8efcb7e54c84b5244c1763a4bf06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be53d6809fd14c1299091d250745d4d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cd9fd81e133c4775bb6f66b326ecb6d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
